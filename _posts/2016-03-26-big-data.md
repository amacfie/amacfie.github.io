---
layout: post
title: What is big data really?
---

**Q**: How big is big?

_A_: Too big to fit in memory or big enough that there are significant gains
from distributed processing.
From "Bayes and Big Data" by Scott et al.:
"A useful definition of 'big data' is data that is too big to comfortably
process on a single machine, either because of processor, memory, or disk
bottlenecks."
Note that it might be possible to fit the data in the memory of a really
expensive server with $$x$$ GB of RAM but it may be cheaper to buy $$n$$
servers with $$x/n$$ GB of RAM each and use distributed big data techniques.

**Q**: Isn't that what databases are for? Databases aren't new.

_A_: Databases are meant for serving queries with low latency.
Although data aggregation is possible, SQL is not designed for complicated
algorithms.
Generally, big data processing is done as an asynchronous job, not in response
to a query, and allows MapReduce operations via general-purpose programming
languages.
These differences have caused the big data ML ecosystem to be built around
tools like Spark rather than databases.

**Q**: Is big data supposed to be an automated backend part of a SaaS app
(operational analytics) or for individual data analysts to use for exploring
data and running ad-hoc models (investigative analytics)?

_A_: Could be either.
Some frameworks are meant more for one use than the other but e.g. Spark
can be used for either.

**Q**: Do I have to know parallel programming to do big data analysis?

_A_: No, frameworks like Hadoop and Spark take care of the low-level details.
You just have to understand how to use MapReduce.

**Q**: What's "Hadoop"?

_A_: Hadoop is a software framework with a number of components which
can be used separately or together.
The components provide functionality for the various tasks required in
distributed big data: a cluster manager, a Map/Reduce API, a distributed
filesystem (like network storage except hosted on a cluster), etc..
Spark is newer than Hadoop and is generally considered to be an improvement,
although it doesn't attempt to cover all the functionality Hadoop does.

**Q**: Hadoop and Spark sound very generic.
They're described with phrases like "programming model", "distributed computing
framework", "data abstraction", etc..
Does it take a lot of custom programming to do practical machine learning
tasks?

_A_: It takes some, but you don't have to implement the machine learning
algorithms yourself.
They're available in libraries like Mahout and MLlib.

**Q**: What if I want an even higher-level interface?

_A_:
Check out
[PredictionIO](https://predictionio.apache.org/gallery/template-gallery/).

